# Финальный алгоритм: Stateful-анализ логов по PID

Этот документ описывает финальный, согласованный stateful-алгоритм для отслеживания использования моделей Ollama. Алгоритм основан на анализе системного журнала (`journalctl`) и использует идентификатор процесса (PID) в качестве ключа для связи событий.

## Ключевые принципы

1.  **PID как ключ сессии:** Каждое событие в логе `journalctl`, относящееся к сервису `ollama`, помечено его PID (например, `ollama[3081]`). Этот PID однозначно идентифицирует сессию работы сервиса от его запуска до остановки и используется для связи всех событий.
2.  **Обратный поиск по требованию:** Вместо того чтобы постоянно хранить сложное состояние, экспортер определяет, какая модель активна, только в момент необходимости (когда видит запрос). Он выполняет быстрый обратный поиск по логу для нахождения события запуска.
3.  **Кэширование для эффективности:** Результат обратного поиска (сопоставление `PID -> model_name`) кэшируется, чтобы избежать повторных дорогостоящих операций поиска в журнале на каждый запрос.

## Шаги алгоритма

1.  **Непрерывный мониторинг лога:**
    *   Экспортер запускает и постоянно читает вывод команды `journalctl -u ollama.service -f -n 0`.

2.  **Триггер: Обнаружение API-запроса:**
    *   Парсер ищет в новых строках лога признаки API-запроса, например, `[GIN]` в сочетании с `POST "/api/chat"` или `POST "/api/generate"`.

3.  **Извлечение PID:**
    *   Из строки запроса извлекается PID сервиса, например, `3081` из `ollama[3081]`.

4.  **Проверка кэша:**
    *   Экспортер проверяет свой внутренний кэш: `cache.get(pid)`. 
    *   Если для данного PID уже есть имя модели, оно используется немедленно (переход к шагу 7).

5.  **Обратный поиск (если в кэше нет данных):**
    *   Если для данного PID модель неизвестна, выполняется команда обратного поиска по журналу, отфильтрованная по этому PID:
        ```bash
        journalctl -u ollama.service --reverse --grep "ollama\[PID\]" --grep "starting llama server" -n 1
        ```
    *   Эта команда находит самую последнюю строку, где сервис с этим PID загружал модель.
## Финальный алгоритм: сопоставление через файловую систему

Этот метод является самым надежным, так как он полностью независим от Ollama API и использует локальные файлы-манифесты как единственный источник правды для связи `blob_digest` с именем модели.

### Часть 1: Построение карты моделей (`model_map`)

Эта задача выполняется один раз при запуске экспортера и может периодически повторяться для обнаружения новых моделей.

1.  **Определить путь к манифестам:** `MANIFESTS_PATH = "/root/.ollama/models/manifests/registry.ollama.ai/library/"`.
2.  **Сканировать директории:**
    -   Экспортер рекурсивно обходит `MANIFESTS_PATH`.
    -   Каждая поддиректория соответствует **имени модели** (например, `llama3`).
    -   Каждый файл внутри поддиректории соответствует **тегу** (например, `latest`).
3.  **Читать и парсить манифесты:**
    -   Для каждого найденного файла-манифеста (например, `/.../llama3/latest`):
        a.  Сформировать полное имя модели: `model_name = "<имя_директории>:<имя_файла>"` (-> `llama3:latest`).
        b.  Прочитать содержимое файла (это JSON).
        c.  Внутри JSON найти массив `layers`.
        d.  Найти в массиве объект, у которого `mediaType` равен `"application/vnd.ollama.image.model"`.
        e.  Из этого объекта извлечь значение `digest` и убрать префикс `sha256:`.
        f.  Создать запись в глобальной переменной `model_map`: `model_map[digest] = model_name`.

**Результат Части 1:** Глобальный словарь `model_map`, содержащий, например, `{"6a0746a1ec1a...": "llama3:latest"}`.

### Часть 2: Парсинг логов и обновление метрик

Этот процесс использует `model_map` и отслеживает состояние последней активной модели.

1.  **Компоненты:**
    -   `model_map`: карта, построенная в Части 1.
    -   `current_active_digest`: переменная для хранения digest'а последней запущенной модели.
2.  **Логика работы:**
    -   Непрерывно читает лог `journalctl -u ollama.service -f`.
    -   **При обнаружении строки `msg="starting llama server"`:**
        a.  Извлекает `blob_digest` из параметра `cmd`.
        b.  Обновляет состояние: `current_active_digest = <извлеченный_digest>`.
        c.  Находит `model_name` в `model_map` по `current_active_digest`. Если не находит, использует `unknown:<digest>`.
        d.  Извлекает параметры запуска (`ctx-size` и т.д.) и обновляет метрики запуска (`ollama_model_start_timestamp`, `ollama_model_context_size` и т.д.) с меткой `model=<model_name>`.
    -   **При обнаружении строки `[GIN] ... "/api/generate"` или `"/api/chat"`:**
        a.  Использует `current_active_digest` для поиска `model_name` в `model_map`.
        b.  Инкрементирует счетчик `ollama_requests_total` с меткой `model=<model_name>`.
